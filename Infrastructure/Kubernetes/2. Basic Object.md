# 기본 오브젝트

## Pod - Container, Label, NodeSchedule

### Container

- `Pod` 안에는 하나의 독립적인 서비스들을 구성할 수 있는 `Container`들이 있으며, 그 컨테이너들은 각각  
  서비스가 연결될 수 있도록 포트를 갖고 있다. 하나의 컨테이너가 두 개 이상의 포트를 가질 수는 있지만,  
  하나의 `Pod`내의 `Container`들이 동일한 포트 번호를 사용할 수는 없다. 즉, **하나의 `Pod` 내에서**  
  **포트 번호는 중복될 수 없다.** 예를 들어, 하나의 `Pod`내에 Container1, Container2가 있다 하자.  
  Container1의 포트는 8000, Container2의 포트는 8080이다. 이러면 Container1에서는 `localhost:8080`으로  
  Container2에 접근할 수 있다.

- `Pod`가 생성될 때 각 `Pod`에는 고유한 IP 주소가 할당된다. 예를 들어, `10.16.36.22`가 할당되었다고 해보자.  
  이 IP는 **K8s `Cluster` 내에서만 유효하다.** 즉, 외부에서는 이 IP로 접근할 수 없다. 또한 `Pod`에 문제가 생기면  
  시스템이 이를 감지하고 해당 `Pod`를 재생성하게 되는데, 할당된 IP는 재생성 시 변경된다. 따라서 이 IP는 휘발성이 있다.  
  아래 파일을 보자.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
    - name: container1
    - image: image/p8000
      ports:
        - containerPort: 8000
    - name: container2
    - image: image/p8080
      ports:
        - containerPort: 8080
```

- 위 yml 파일은 pod-1이라는 이름을 가진 `Pod`를 만들며, 8000번 포트를 가진 Container1과 8080번 포트를 가진  
  Container2라는 두 개의 `Container`를 만든다.

### Label

- `Label`은 `Pod` 뿐만 아니라, 모든 `Object`에 적용할 수 있는데, 주로 `Pod`에서 가장 많이 사용한다.  
  `Label`을 사용하는 이유는 목적에 따라 `Object`들을 분류하고, 분류된 `Object`들끼리만 골라 연결하기 위함이다.

- `Label`은 `Key: Value` 형식으로 구성되며 하나의 `Pod`에는 여러 개의 `Label`을 달 수 있다.

![picture 10](../../images/K8S_LABEL.png)

- 위와 같이 `Label`이 구성되어 있다고 하자. `type:web`, `type:db`, `type:server`의 `Label`이 있으며,  
  각각 개발 환경(`lo:dev`)과 상용 환경(`lo:production`)에 있다.

- 만약 이러한 상황에서 웹 개발자가 _웹 화면반 보고 싶다_ 라 한다면, `type:web`의 `Label`을 가진 `Pod`들을  
  `Service`에 연결해서 이 `Service`의 정보를 알려주면 된다. 같은 원리로 _상용 환경을 담당하는 운영자_ 에게는  
  `lo:production`의 `Label`을 가지는 `Pod`들을 `Service`에 연결해서 이 `Service`의 정보를 알려주면 된다.

- 이렇게 사용 목적에 따라 `Label`을 등록하면, 경우에 따라 원하는 `Pod`만 사용할 수 있다.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    type: web
    lo: dev
spec:
  containers:
    - name: container
    - image: image/init
```

- 위 yml 파일 처럼 `Pod`를 만들 때 lables에 key-value 쌍으로 `Label`을 지정할 수 있으며,  
  추후 `Service`를 만들 때는 아래 yml 파일처럼 특정 `Label`을 가진 `Pod`들을 가져와 생성할 수 있다.  
  (아래 파일에는 `type:web`의 `Label`을 가지는 `Pod`들을 선택해 `Service`를 만든다.)

```yml
apiVersion: v1
kind: Service
metadata:
  name: service-1
spec:
  selector:
    type: web
  ports:
    - port: 8080
```

### Node Schedule

- `Pod`는 결국 여러 `Node`들 중에서 한 `Node`에 올라가져야 한다. 이 방법에 대해서, 운영자가 직접 `Node`를  
  선택하는 방법과 K8S가 자동으로 지정해주는 방법이 있다.

#### Node 직접 선택하기

- 이전에 `Pod`에 `Label`을 단 것처럼 `Node`에 `Label`을 달고, `Pod`를 만들 때 `Node`를 지정할 수 있다.  
  아래 파일 내용을 보자.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
spec:
  nodeSelector:
    hostname: node1
  containers:
    - name: container
      image: image/init
```

#### K8S의 Scheduler가 판단해 지정해주기

- `Node`에는 전체 사용 가능한 자원량이 있다. 대표적으로 메모리의 CPU를 생각해보자.  
  만약 Node1에 2개의 `Pod`가 들어가 있어서 남은 메모리가 1GB이고, Node2에는 아무런 `Pod`가 없어  
  사용 가능한 메모리가 3.7GB 라 해보자.

- 위 상황에서 2GB의 메모리가 필요한 `Pod`를 만들어야 한다 해보자. 이때, K8S가 알아서 더 여유 메모리가 많은  
  `Node`인 Node2로 스케쥴링 해준다.

- 참고로 `Pod`에 메모리 사용량을 명시해주는 이유는, 만약 `Pod`가 사용할 메모리량이 명시되어 있지 않다면 `Pod`내의  
  서비스에서 부하가 생길 때 소속된 `Node`의 자원을 무한정 사용하려 할 것이며, 이렇게 되면 해당 `Node`에 있는 다른  
  `Pod`들은 자원이 없어서 결국 모두 죽게 된다.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-4
spec:
  containers:
    - name: container
      image: image/init
      resources:
        requests:
          memory: 2Gi
        limits:
          memory: 3Gi
```

- `Pod`를 만드는 위의 yml 파일은 해당 `Pod`가 2Gi의 메모리를 요구하며, 최대 3Gi의 메모리를 허용한다는 내용을  
  담고 있다. limits에 대해 조금 더 살펴보자.

  - memory: 초과시 해당 `Pod`가 종료된다.
  - cpu: 초과시 request로 낮춘다. Over 시 종료되지 않는다.

<hr/>

## Service - ClusterIP, NodePort, LoadBalancer

- `Service`는 Cluster 내에서 기본적으로 자신의 Cluster IP를 갖고 있다. 그리고 `Service`를 `Pod`에  
  연결시켜 놓으면 `Service`의 IP를 통해서도 `Pod`에 접근할 수 있게 된다. 하지만 `Pod`에도 똑같이 `Cluster`  
  내에서 접근할 수 있는 IP가 있다. 그럼 왜 굳이 `Service`를 달아서 `Service`의 IP를 통해 접근할까?

- 그 이유는 `Pod`는 K8S에서 시스템 장애, 성능 장애 등의 이유로 언제든지 죽을 수가 있고, 그럴 경우 재생성될 수 있는  
  오브젝트이기 때문이다. `Pod`의 IP는 재생성되면 변경되므로 `Pod`의 IP는 신뢰성이 떨어진다.  
  하지만 `Service`는 사용자가 직접 지우지 않는 한 삭제되거나 재생성되지 않는다. 그래서 `Service`의 IP로 접근하면  
  항상 해당 `Service`에 연결되어 있는 `Pod`에 접근할 수 있다.

### ClusterIP

- `Service`에는 여러 종류가 있는데, 종류 별로 `Pod`에 대한 접근을 지원하는 방식이 다르다.  
  그중 가장 기본적인 방식은 `ClusterIP` 방식이다. `ClusterIP` 방식의 `Service`에 부여되는 IP는  
  `Pod`에 부여되는 IP와 마찬가지로 `Cluster` 내의 모든 오브젝트는 접근할 수 있지만, 외부에서는 접근할 수 없다.

- `Service`를 만드는 yml 파일을 보자.

```yml
apiVersion: v1
kind: Service
metadata:
  name: service-1
spec:
  selector:
    app: pod
  ports:
    - port: 9000
      targetPort: 8080
  type: ClusterIP # 기본값: ClusterIP
```

- 위 파일의 내용은 9000번 포트로 `Service` IP로 요청이 오면, `Pod`의 8080번 포트로 연결된다는 뜻이다.  
  `Pod`의 yml도 보자. 8080번 포트를 사용한다.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    app: pod
spec:
  containers:
    - name: container
      image: image/app
      ports:
        - containerPort: 8080
```

![picture 11](../../images/SERVICE_CLUSTER_IP.png)

### NodePort

- `NodePort` 타입의 `Service`를 만들어도, `Service`에는 Cluster IP가 할당되어 `ClusterIP` 타입의  
  `Service`의 기능이 포함되어 있다.

- `NodePort`만의 큰 특징은 K8S Cluster 내에 할당되어 있는 모든 `Node`에 동일한 포트 번호가 할당되어  
  외부로부터 어떤 `Node`건 간에 해당 `Node`의 IP와 포트로 접속을 하면 `Service`에 연결된다.  
  그럼 `Service`는 자신에게 연결되어 있는 적절한 `Pod`에게 트래픽을 전달해준다.

- 주의할 점은 `Pod`가 있는 `Node`에게만 포트가 할당되는 것이 아니라, **모든 `Node`에 포트가 할당된다.**  
  yml 내용을 보자.

```yml
apiVersion: v1
kind: Service
metadata:
  name: service-2
spec:
  selector:
    app: pod
  ports:
    - port: 9000
      targetPort: 8080
      nodePort: 30000
  type: NodePort
```

> spec.ports.nodePort는 Optional 한 속성이며, 값을 주지 않으면 30000~32767번 사이의  
> 임의의 값이 `Node`의 포트로 할당된다.

- 한 가지 특징을 더 보자.  
  Node1 `Node`에 이름이 Pod1인 `Pod`가 있고, Node2 `Node`에 이름이 Pod2인 `Pod`가 있다 하자.  
  이 상태에서 외부에서 Node1의 IP로 접근하더라도 `Service`는 Node2에 있는 `Pod`에게 트래픽을 전달할 수 있다.  
  **`Service`의 입장에서는 어떤 `Node`로부터 온 트래픽인지 관계없이 알맞은 `Pod`를 찾아 트래픽을 전달하기 때문이다.**  
  그런데, 만약 yml 파일에 `externalTrafficPolicy: Local` 속성을 지정하면 특정 `Node`의 IP를 통해 들어온  
  트래픽을 `Service`는 해당 `Node` 내의 `Pod`에게만 전달할 수 있다. 즉, Node1의 IP를 타고 들어온 트래픽을  
  `Service`가 Node2 내의 `Pod`에게 전달할 수 없다는 뜻이다.

![picture 12](../../images/SERVICE_NODEPORT.png)

### Load Balancer

- `Load Balancer` 타입으로 `Service`를 만들면 기본적으로는 `NodePort`의 성격을 그대로 갖고 있다.  
  추가적으로 `Load Balancer`가 생겨서, 해당 `Load Balancer`의 IP로 트래픽이 들어오면, `Load Balancer`가  
  트래픽을 분산시켜준다.

- 참고로 `Load Balancer`에 접근하기 위한 외부 접속 IP 주소는 기본적으로 생기지 않는다.  
  별도로 외부 접속 IP를 할당해주는 플러그인을 설치해야 할당받을 수 있다.

```yml
apiVersion: v1
kind: Service
metadata:
  name: service-3
spec:
  selector:
    app: pod
  ports:
    - port: 9000
      targetPort: 8080
  type: LoadBalancer
```

### 정리

- 이제 각각의 서비스 타입을 어떤 상황에 사용해야 하는지 알아보자.

#### ClusterIP

- 외부에서는 접근할 수 없고, `Cluster` 내에서만 사용하는 IP이다. 따라서 이 IP에 접근할 수 있는 주체는  
  인가된 사용자(운영자 등)일 수밖에 없으며 주로 내부 dashboard를 관리하거나, `Pod`의 서비스 상태를  
  디버깅할 때 사용한다.

#### NodePort

- `NodePort`의 특징은 `Node`의 IP를 통해 `Service`에 접근하고, 결국 `Pod`에도 접근할 수 있다는  
  점인데, 대부분 `Node`의 IP는 보안적으로 내부망에서만 접근할 수 있도록 네트워크를 구성하기 때문에  
  `Cluster` 밖에 있지만, 그래도 내부망에서 접근할 때만 쓰인다. 또한 일시적으로 외부 연동 용으로도 사용한다.

#### Load Balancer

- 실제로 외부에 `Service`를 노출시키기 위해 사용하는 타입이다. 그래야 내부 IP가 노출되지 않고 외부 IP를  
  통해 안정적으로 서비스를 노출시킬 수 있다. 따라서 `Load Balancer`는 외부에 시스템을 노출하는 용도로 쓰인다.

<hr/>

## Volume - emptyDir, hostPath, PV/PVC

### emptyDir

- emptyDir은 컨테이너들끼리 데이터를 공유하기 위해 `Volume`을 사용하는 것이며 최초로 이 `Volume`이  
  생성될 때 아무런 데이터도 없이 비어있기 때문에 emptyDir이라는 이름이 생겼다. Container1과 Container2가  
  하나의 `Volume`에 연결되어 있고, Container1은 웹을, Container2는 백엔드 단을 처리한다 해보자.  
  그러면 Container1이 웹을 통해 받은 어떤 파일을 mount된 `Volume`에 저장해 놓고, 백엔드 단에 있는  
  Container2 역시 이 `Volume`을 mount해놓으면 Container1과 Container2가 마치 이 `Volume`을  
  자신의 로컬에 있는 파일처럼 사용하기에 두 컨테이너가 굳이 파일을 주고받을 필요 없이 편리하게 사용할 수 있다.

- 또한 이 `Volume`은 `Pod` 안에 생성되기 때문에 `Pod`가 문제가 발생해 재생성되면 `Volume` 역시  
  삭제되고, 생성 시 새롭게 다시 만들어진다. 따라서 이 `Volume`에는 꼭 일시적 사용 목적을 가진 데이터들만  
  넣는게 좋다.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-1
spec:
  containers:
    - name: container1
      image: image/init
      volumeMounts:
        - name: empty-dir
          mountPath: /mount1
    - name: container2
      image: image/init
      volumeMounts:
        - name: empty-dir
          mountPath: /mount2
  volumes:
    - name: empty-dir
      emptyDir: {}
```

- 위 yml 파일에는 두 개의 `Container`가 있으며, 둘 다 `Volume`을 mount하고 있다.(volumeMounts)  
  mount한 위치를 보면 Container1은 `/mount1`에, Container2는 `/mount2`에 mount 했다.  
  이때, volumeMounts.name에서는 똑같은 `Volume`의 이름을 지정하고 있기 때문에 `Container`마다 자신이  
  원하는 위치에 `Volume`을 mount하지만, 결국 하나의 `Volume`을 mount하는 것이다.

![picture 13](../../images/VOLUME_EMPTYDIR.png)

### hostPath

- hostPath는 이름 그대로 하나의 Host, 즉 `Pod`들이 올라가져 있는 `Node`의 path를 `Volume`으로써  
  사용하는 것인데, emptyDir과 다른점은 `Node`의 path를 각각의 `Pod`들이 mount한 후 공유하기 때문에  
  `Pod`들이 죽어도 `Node`에 있는 데이터들은 사라지지 않는다. 나름 좋아보일 수 있지만, `Pod` 입장에서 보면  
  한 가지 큰 문제점이 있다.

- 바로 **`Pod`가 죽었을 때, 꼭 다시 죽은 `Node`에서 재생성 될 것이라는 보장이 없다는 점** 이다.  
  만약 기존에 Node1에 있던 `Pod`가 죽어 재생성될 때 스케쥴러가 자원 상황을 보고 Node1가 아닌 Node2에  
  재생성시킨다면, `Volume`은 Node1에 있기에 `Volume`을 mount할 수 없게 된다.

- 이를 위한 해결책으로 새로운 `Node`가 추가될 때마다 똑같은 이름의 경로를 만들어서 직접 `Node`에 있는  
  path끼리 mount를 시켜주면 된다. 이렇게 구성하는게 어렵지는 않지만, K8S가 제공하는 기능이 아니며  
  운영자가 직접 구현해야 한다. 이는 자동화가 아닌 사람이 하는 작업이라 human fault가 일어날 여지가 많다.

- 그럼 이 hostPath는 어떨 때 사용해야 할까? 각각의 `Node`에는 기본적으로 각 `Node` 자신을 위해서  
  사용되는 파일들이 있다. 시스템 파일들, 또는 여러 설정 파일들이 있을 수 있다.  
  `Pod` 자신이 할당되어 있는 host, 즉 `Node`에 데이터를 읽거나 써야할 때 사용하면 된다.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-2
spec:
  containers:
    - name: container
      image: image/init
      volumeMounts:
        - name: host-path
          mountPath: /mount1
  volumes:
    - name: host-path
      hostPath:
        path: /node-v
        type: Directory
```

- 한 가지 주의점으로는 volumes.hostPath.path에 명시된 path는 `Pod`가 만들어지기 전에  
  이미 있어야 한다. 그렇지 않으면 `Pod` 생성 시 에러가 발생한다.

![picture 14](../../images/VOLUME_HOSTPATH.png)

### PV/PVC(Persistent Volume / Persistent Volume Claim)

- PV, PVC는 `Pod`에 영속성 있는 `Volume`을 제공하기 위한 개념이다.  
  즉 `Volume`안에 git, aws가 제공하는 서비스 등을 연결해 사용할 수 있다.  
  이때 `Pod`는 `PV`와 직접적으로 연결하지 않고, `PVC`를 통해 `PV`와 연결하게 된다.  
  직접 `PV`에 연결하지 않는 이유를 보자.

- K8S는 `Volume`의 사용에 있어 User와 Admin의 두 개 영역을 분리한다.  
  Admin은 K8S를 담당하는 K8S 운영자가 되며, User는 `Pod`에 서비스를 만들고, 배포를 담당하는  
  서비스 담당자이다. `Volume`안에는 위에서 봤듯이 여러 서비스를 둘 수 있으며, 서비스마다 각각  
  설정 방법도 다르다. 예시를 보자.

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-01
spec:
  nfs:
    server: 192.168.0.xx
    path: /sda/data
  iscsi:
    targetPortal: 163.xx.xx.xx
    iqn: iqn.200.qnap:...
    lun: 0
    fsType: ext4
    readOnly: no
    chapAuthSession: true
  gitRepo:
    repository: github.com/..
    revision: master
    directory: .
```

- 위 파일을 보고 알 수 있듯이 정말 설정값들이 모두 다르다.  
  이렇게 `PV`를 전문적으로 관리하는 Admin이 `PV`를 만들어 놓으면 User는 이를 사용하기 위해 `PVC`를  
  만들어야 하는데, `PVC`를 만들기 위한 파일을 보자.

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-01
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ""
```

- 위 파일을 보면 읽기 및 쓰기가 되며 용량이 1Gi인 볼륨을 할당해달라고 요청한다.

- 마지막으로 `Pod`를 만드는 부분을 보자.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-3
spec:
  containers:
    - name: container
      image: image/init
      volumeMounts:
        - name: pvc-pv
          mountPath: /volume
  volumes:
    - name: pvc-pv
      persistentVolumeClaim:
        claimName: pvc-01
```

- volumes.persistentVolumeClaim.claimName 부분을 보면 앞서 만든 `PVC` 이름을 넣는 것을 볼 수 있다.

- 정리하자면 Admin이 `PV`를 정의하면 User가 `PVC`를 만들고, 그렇게 되면 K8S가 `PVC` 내용에 맞는 적절한  
  `PV`로 연결을 해준다. 그리고나서 `Pod`를 만들 때 이 `PVC`를 사용하면 된다.

![picture 15](../../images/VOLUME_PV_PVC.png)

<hr/>

## ConfigMap, Secret - Env, Mount

- 개발 환경과 프로덕션 환경의 K8S 오브젝트가 있다고 해보자. 데이터베이스 설정, 프로파일 설정 등 다양한 설정들이  
  이 두 환경에서 차이날 것이다. 이 값들은 보통 서비스의 Image내에 들어있기 때문에 이 값을 바꾼다는 것은 곧  
  개발 및 프로덕션 환경의 Image를 각각 관리해야 한다는 것을 의미한다. 이러한 값 몇 개 때문에 매우 클 수도 있는  
  Image를 관리해야 한다는 것은 꽤나 부담되는 일이다.

- 위와 같은 이유로 **환경에 따라 변하는 값들은 외부에서 결정할 수 있게 하는데,** 이를 도와주는 수단이 바로  
  `ConfigMap`과 `Secret`이라는 오브젝트이다. 분리할 값들을 key-value 쌍으로 `ConfigMap`에 모으고,  
  Secret Key와 같이 보안성이 중요한 값들은 `Secret`에 담아 관리한다.

- 추후 `Pod` 생성 시 컨테이너에 `ConfigMap`과 `Secret`을 연결하여 `Container`의 환경 변수로 사용할 수 있다.

- `ConfigMap`과 `Secret`을 만들 때에는 데이터를 상수(literal)로 넣을 수도 있고, 파일로 관리할 수도 있다.  
  또한 파일을 사용한다면 환경 변수로 사용하는 것이 아닌, `Volume`에 mount해서 사용할 수도 있다.

- 위에서 `Secret`은 보안적인 값들을 넣는 데 사용한다고 했다. `ConfigMap`과 key-value의 쌍으로  
  값을 넣는 다는 것은 동일하지만, `Secret`에는 Base64 encoding된 값을 넣어야 한다. 그 후 `Pod`에서  
  사용해야 할때 자동으로 decoding되어 주입된다. 또한 일반적인 `ConfigMap`은 K8S DB에 저장되지만  
  `Secret`의 내용들은 메모리에 저장된다. 또한 `ConfigMap`은 무한대인 반면, `Secret`에는  
  최대 1MB의 내용들을 넣을 수 있다.

### 환경 변수를 상수로 관리하기 - Env(Literal)

```yml
# yml file for ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-dev
data:
  SSH: 'false'
  User: dev

# yml file for Secret
apiVersion: v1
kind: Secret
metadata:
  name: secret-dev
data:
  Key: thisKeyWillBeBase64Encoded

# yml file for Pod
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
    - name: container
      image: image/init
      envFrom:
      - configMapRef:
        name: cm-dev
      - secretRef:
        name: secret-dev
```

### 환경 변수를 파일로 관리하기 - Env(File)

- `file.txt`라는 파일 자체를 `ConfigMap`에 담을 수 있다. 이때 **파일명이 key가 되며, 그 파일의 내용이 value가 된다.**  
  상수와 마찬가지로 파일 내의 내용도 `Secret`의 경우에는 Base64 encoding되어 들어간다.

- `ConfigMap`을 파일로 만드는 것은 대시보드에서 지원하지 않기에 직접 Master에 접속하여 생성해야 한다.

```sh
# create ConfigMap with the name of cm-file
kubectl create configmap cm-file --from-file=./file-s.txt

# create Secret with the name of sec-file
kubectl create secret generic sec-file --from-file=./file-c.txt
```

- `Secret`과 `ConfigMap`은 CLI로 만들었으니, `Pod`를 만드는 yml 파일을 보자.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: file
spec:
  containers:
    - name: container
      image: image/init
      env:
        - name: file-c
          valueFrom:
            configMapKeyRef:
              name: cm-file
              key: file-c.txt
        - name: file-s
          valueFrom:
            secretKeyRef:
              name: sec-file
              key: file-s.txt
```

### 파일을 `Volume`에 마운팅하기

- `ConfigMap`과 `Secret`을 만드는 것까지는 동일하나, `Pod` 생성 파일에 mount할 경로를 지정하고,
  지정된 mount path 내에 파일을 mount할 수 있다.

```yml
apiVersion: v1
kind: Pod
metadata:
  name: mount
spec:
  containers:
    - name: container
      image: image/init
      volumeMounts:
        - name: file-volume
          mountPath: /mount
      volumes:
        - name: file-volume
      configMap:
        name: cm-file
```

- File을 mount하는 방식과 환경변수로 주입하는 방식에는 큰 차이가 있다.  
  `Pod` 생성 후 `ConfigMap`의 내용을 변경하면 어떻게 될까?  
  환경변수 방식은 한 번 주입되면 끝이기 때문에 `ConfigMap`내의 데이터가 변해도  
  `Pod`내의 `Container`에는 아무런 영향이 없다. 변경 사항을 반영하는 방법은 오로지 `Pod`를  
  재생성하는 방법 뿐이다. 반면 mount 방식은 _원본과 연결_ 시켜 놓기 때문에 `ConfigMap`의 내용이  
  변경되면, 해당 `Pod`에 mount된 내용도 함께 변한다.

<hr/>

## Namespace, ResourceQuota, LimitRange

- 이들을 사용하기에 앞서, 왜 필요한지 알아보자.

- Kubernetes Cluster에는 전체 사용할 수 있는 자원이 있다.  
  일반적으로 메모리나 CPU가 있을 것이다.  
  이러한 `Cluster`내에는 여러 `Namespace`를 만들 수 있으며, 하나의 `Namespace` 내에는  
  하나 이상의 `Pod`가 있다. 각 `Pod`는 필요한 자원을 `Cluster` 자원을 공유받아 사용하는데, 만약  
  하나의 `Namespace`내의 `Pod`가 이 `Cluster`의 가용 자원을 모두 사용해버리면 자원이 부족해  
  다른 `Pod`를 사용할 수 없게 된다.

- 위의 문제를 해결하기 위해 `ResourceQuota`가 존재하는데, 이를 `Namespace`마다 지정해주면  
  각 `Namespace`마다 차지할 수 있는 자원의 한계치를 설정해줄 수 있다. 만약 어떤 `Namespace`에  
  하나의 `Pod`가 있고, 이 `Pod`가 자원 사용량이 지정된 `ResourceQuota`만큼 사용되면 그  
  `Namespace`에는 다른 `Pod`들이 더 이상 들어올 수 없게 된다. 이런 현상을 관리하기 위해서는  
  `LimitRange`를 지정할 수 있다. 만약 `Pod`의 크기가 지정된 `LimitRange`를 넘어선다면  
  해당 `Pod`는 `Namespace`에 들어올 수 없다.

- 참고로 `ResourceQuota`와 `LimitRange`는 `Namespace` 뿐만 아니라 `Cluster` 자체에도  
  명시할 수 있어 총 자원 사용량을 지정할 수도 있다.

### Namespace

- 한 `Namespace` 내에서는 같은 타입의 오브젝트의 이름이 중복될 수 없다.  
  예를 들어 이름이 Pod1인 `Pod`가 두개 이상일 수 없다.

- 또한 `Namespace`의 대표적인 특징은 다른 `Namespace`내에 있는 자원들과 분리되어 관리된다는 점이다.  
  예를 들어 Namespace2의 `Service`는 Namespace1의 `Pod`에 연결할 수 없다.

> 내부 IP 주소를 사용해서 연결을 할 수는 있다.

- 대부분의 자원들은 해당 자원을 만든 `Namespace` 내에서만 사용할 수 있다.  
  물론 `PV`나 `Node`와 같이 모든 `Namespace`에서 공용으로 사용되는 오브젝트도 있다.
- 마지막으로 `Namespace`를 지운다면 해당 `Namespace`내의 모든 자원도 함께 삭제된다.

- yml 파일을 보자.

```yml
# yml file for Namespace1
apiVersion: v1
kind: Namespace
metadata:
  name: namespace1

# yml file for Pod inside Namespace1
apiVersion: v1
kind: Pod
metadata:
  name: name-pod1
  namespace: namespace1
  labels:
    nm: pod1
spec:
  containers:
   #..

# yml file for Namespace2
apiVersion: v1
kind: Namespace
metadata:
  name: namespace2

# yml file for Service inside Namespace2
apiVersion: v1
kind: Service
metadata:
  name: svc-1
  namespace: namespace2
spec:
  selector:
    nm: pod1
  ports: #..
```

- 위 파일에서 svc-1 `Service`는 `nm:pod1` 라벨을 가진 `Pod`를 찾지만, 이들은 서로 다른  
  `Namespace`에 있기 때문에 찾을 수 없다.

### ResourceQuota

- `ResourceQuota`는 `Namespace`의 자원 한계를 설정하는 오브젝트이다.
  한 가지 주의점은 `ResourceQuota`가 지정된 `Namespace` 내에 `Pod`를 만들 때에는  
  `Pod`도 사용할 리소스량을 명시해야 한다.

- 아래 예시처럼 Memory나 CPU외에 Storage 및 오브젝트들(`Pod`, `Service`, `ConfigMap`, `PVC`, ..)의  
  숫자도 제한할 수 있다.

```yml
# yml file for Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: namespace1

# yml file for ResourceQuota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq-1
  namespace: namespace1
spec:
  hard:
    requests.memory: 3Gi
    limits.memory: 6Gi

# yml file for Pod
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  namespace: namespace1
spec:
  containers:
    - name: container
      image: image/app
      resources:
        limits:
          memory: 2Gi
        requests:
          memory: 4Gi
```

### LimitRange

- `LimitRange`는 각각의 `Pod`가 해당 `Namespace`에 들어올 수 있는지 자원을 체크해준다.

![picture 17](../../images/LIMITRANGE.png)

> maxLimitRequestRatio: 최소값과 최대값의 비율,  
> 위 사진에서 `Pod2`가 들어올 수 없는 이유가 된다.(4 이기 때문)
> defaultRequest, default를 지정하면 `Pod`에 아무런 자원량이 명시되어 있지 않을 때  
> 자동으로 여기에 지정된 자원량만큼 할당된다.

- yml 파일을 보자.

```yml
apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange-1
  namespace: namespace1
spec:
  limits:
    - type: Container # type마다 지원되는 옵션들이 다르다.
      min:
        memory: 1Gi
      max:
        memory: 4Gi
      defaultRequest:
        memory: 1Gi
      default:
        memory: 2Gi
      maxLimitRequestRatio:
        memory: 3
```

<hr/>
