# Proximity Service

- 이번 장에서는 proximity service를 설계해보자. Proximity service는 주로 음식점, 호텔, 영화관, 박물관 등 주변 장소들을  
  표현하는 데에 사용된다. 이런 proximity service는 주변에 있는 가장 좋은 음식점, 가장 가까운 주요소 등을 알려주는 데에 가장  
  큰 역할을 하는 컴포넌트가 된다.

## 문제 이해 및 설계 범위 확정

- Yelp의 proximity service는 가까운 장소들을 알려주는 것 외에도 많은 다양한 기능들을 제공한다.  
  따라서 이런 시스템 설게 인터뷰에서는 문제를 이해하고 설계 범위를 확실히 짚고 넘어가는 것이 좋다.

- 이번 장에서 설계할 proximity service의 요구사항은 아래와 같다.

  - 사용자의 위도, 경도 및 특정 반경 내의 모든 장소들을 알려준다.
  - 장소의 소유자들은 장소를 추가, 삭제 또는 갱신할 수 있으며 이러한 수정된 정보는 실시간으로 반영되지 않아도 된다.
  - 사용자들은 특정 장소에 대해 세부 정보를 알 수 있다.
  - DAU는 100M(100만) 이고 등록되어 있는 장소는 200M(200만) 개 이다.

- 위의 요구사항에 더불어, 아래의 추가적인 사항들도 적용해야 한다.

  - 낮은 latency: 사용자들은 주변 장소들을 빠르게 볼 수 있어야 한다.
  - Data privacy(데이터 안전성): 위치 정보는 꽤나 민감한 데이터이다. 따라서 LBS(Location-Based Service)를 설계할 때는 항상  
    사용자의 정보 보호를 위한 고민을 해야 한다. GDPR, CCPA 등 표준화된 데이터 보호 정책을 따르도록 해야 한다.
  - 높은 가용성(high availability)과 확장성(scalability): 시스템은 유명한 지역에서 트래픽이 치솟아도 서비스를 안정적으로 제공해야 한다.

- QPS를 계산하는 과정은 아래와 같다.

  - 하루는 총 `24 * 60 * 60 = 86400` 초이고, 이를 계산하기 쉽게 하기 위해 `10^5`로 나타내보자.
  - 사용자가 하루에 5번의 질의를 한다고 해보자.
  - 그러면 QPS는 `100M * 5 / 10^5 = 5000`이다.

---

## 개략적 설계안 제시 및 동의 구하기

- 이번 장에서는 아래의 컴포넌트들을 주로 다뤄볼 것이다.

  - API 설계
  - 개략적 설계
  - 주변 장소들을 찾기 위한 알고리즘
  - 데이터 모델

### API 설계

- RESTful API의 컨벤션을 따라 API의 개략적 설계를 진행해보자.  
  그중 하나는 `GET /v1/search/nearby`이다.  
  이 API endpoint는 특정 검색 조건에 따른 주변 장소들을 반환한다. 실세계 애플리케이션에서 이런 경우에 반환되는 정보는 주로 페이징 처리  
  (pagination)되어 있다. 페이징 처리 방식에 대해 다루진 않을 것이지만, 한번 쯤 다뤄볼만 하다.

- 이 API의 request parameter는 아래와 같다.

  - latitude(필수): decimal, 주어진 위치의 위도
  - longitude(필수): decimal, 주어진 위치의 경도
  - radius(선택, 기본 5000): int, 검색할 반경, m단위

- 반환되는 JSON 스키마는 대략 아래와 같다.

```json
{
	"total": 10,
	"business": [{"business object"}]
}
```

- 위의 `business object`는 검색 결과 페이지를 렌더링하기 위해 필요한 모든 정보들을 담는 객체이다.  
  하지만 장소의 상세 페이지에서는 텍스트 뿐만 아니라 사진, 리뷰, 별점 등 추가적인 정보들이 분명히 필요할 것이다.  
  따라서 사용자가 특정 장소의 상세 페이지에 들어가면 호출할 API를 정의해보자.

- 상세 페이지에 관련된 API들의 목록은 아래와 같다.

  - `GET /v1/businesses/:id`: 특정 정보의 모든 상세 정보 조회
  - `POST /v1/businesses`: 새로운 장소 생성
  - `PUT /v1/businesses/:id`: 특정 장소 정보 수정
  - `DELETE /v1/businesses/:id`: 특정 장소 삭제

### 데이터 모델

- 아번에는 read/write 비율과 데이터베이스 스키마를 지정해보자.

#### Read/Wrtie 연산 비율

- 아래의 두 가지 기능이 가장 자주 사용되기에 read 연산의 비중이 꽤나 높을 것이다.

  - 주변 장소 검색
  - 특정 장소의 상세 정보 조회

- 반면, 장소를 추가, 삭제하거나 장소 정보를 수정하는 것은 검색 만큼 자주 일어나는 작업은 아니다.

- 이런 상황에서 대규모 시스템을 위해서라면 MySQL과 같은 관계형 데이터베이스를 사용하는 것이 좋은 선택지 중 하나이다.

#### 데이터 스키마

- 가장 중요하게 봐야할 것은 장소들을 저장하는 business와 위치를 위한 geospatial(geo) index table이다.

##### Business table

- Business table은 특정 장소에 대한 모든 상세 정보들을 저장한다.

| column name | key |
| :---------- | :-- |
| business_id | PK  |
| address     | -   |
| state       | -   |
| country     | -   |
| latitude    | -   |
| longitude   | -   |

##### Geo index table

- Geo index table은 지리적 정보에 대한 연선을 하기 위해 최적화된 테이블이다. Geo hash에 대한 지식이 필요하므로, 이후에 자세히 보자.

### 개략적 설계

- 아래 그림이 이번 proximity service의 개략적 설계안이다.  
  시스템을 구성하는 두 가지 주요 컴포넌트로는 LBS(Location-Based Service)와 장소 관련 요청을 처리하는 서비스가 있다.

![picture 10](/images/SDI2_PS_1.png)

- 위 설계안의 각 컴포넌트들을 차례로 살펴보자.

  - Load Balancer: 들어오는 요청(트래픽)을 여러 개의 서비스에 골고루 분산시켜준다. 일반적으로는 하나의 DNS Entry point를 두고  
    해당 entry point로 오는 모든 요청을 URL Path에 따라 적절한 서비스로 routing한다.

  - LBS(Location-Based Service): LBS는 주어진 위치와 반경에 따라 주변 장소들을 찾아주는 이 시스템의 핵심 컴포넌트 중 하나이다.  
    LBS의 특징은 아래와 같다.

    - Write 연산이 없는 read 연산량을 매우 많이 처리한다.
    - 밀집된 지역에 트래픽이 치솟는 시간이 있기에 QPS가 굉장히 높다.
    - 수평적 확장(horizontal scale)을 위해 stateless하다.

  - Business Service: 이 서비스는 크게 아래의 두 가지 요청을 처리한다.

    - 장소의 주인이 장소를 등록, 정보 수정, 삭제하는 요청. 이 요청은 주로 write 연산이며 QPS는 그렇게 높지 않다.
    - 사용자들이 특정 정보의 상세 정보를 조회하는 요청. QPS는 peak 시간대에 높다.

  - Database Cluster: Database cluster는 primary-secondary setup으로 구성되어 있다. Primary database는 모든  
    write 연산을 처리하고, 여러 개의 replica들이 read 연산을 처리하기 위해 사용된다. 새로운 데이터는 우선 primary database로  
    전달되어 처리되고, 이후 replica들 각가에 복제(replication)된다. 복제 과정에는 delay가 있을 수 있기 때문에 primary database에  
    있는 새로운 데이터와 LBS의 조회 결과가 다른 상황이 발생할 수 있다. 이러한 데이터의 일관성 불일치는 장소 정보가 실시간 처리되지 않아도  
    된다는 요구사항 덕분에 크게 신경쓰지 않아도 된다.

  - LBS와 Business Service의 확장성: LBS와 Business Service는 모두 stateless service이기 때문에 트래픽이 치솟는 상황이라면  
    쉽게 서버를 추가하고, 사용량이 적다면 줄일 수 있다. 만약 클라우드 상에 시스템을 구축한다면 다중 region 또는 다중 AZ를 활용해  
    가용성을 더 높일 수 있다.

### 주변 장소들을 찾기 위한 알고리즘

- 실세계에서 많은 회사들은 Redis의 Geohash, PostGIS extension이 적용된 PostgreSQL 등의 geospatial database를 사용한다.  
  물론 이런 인터뷰 과정에서 geospatial database에 대해 깊게 알아야 할 필요는 없다. 다만, 이것보다 geospatial index의 동작 방식을  
  이야기함으로써 문제 해결 능력과 기술적 지식을 어필할 수 있을 것이다.

- 주변 장소들을 가져오기 위해 사용할 수 있는 선택지 몇 개를 살펴보자.

##### Two-dimensional Search

- 가장 직관적이지만 순진한 방법으로 정해진 반경에 따라 위치로부터 원을 그리고, 해당 원 내부에 있는 모든 장소들을 찾을 수 있다.  
  이 과정은 아래의 pseudo SQL 문으로 표현할 수 있다.

```sql
SELECT business_id, latitude, longitude
FROM business
WHERE (latitude BETWEEN {:my_lat} - radius AND {:my_lat} + radius)
	AND (longitude BETWEEN {:my_long} - radius AND {:my_long} + radius);
```

- 위 query는 테이블 전체를 scan해야 하기 때문에 효율적이지 못하다. 만약 latitude와 longitude column에 대해 index를 추가하면 조금 더  
  효율적이게 될까? 정답은 꼭 그렇지만은 않다. 근본적인 문제는 데이터가 2차원이고 각 차원에 대해 데이터베이스가 반환하는 데이터가 굉장히 클 수 있다는  
  것이다. index를 추가하면 특정 위도 범위 사이에 있는 장소와 특정 경도 범위 사이에 있는 장소들을 가져오는 것은 빠를 수 있지만, 이 결과를 갖고  
  intersect(교집합) 연산을 해야한다는 것이 문제가 된다. 이는 각 dataset이 많은 데이터를 가질 수 있기 때문에 비효율적이다.

- 이 방법의 문제점은 데이터베이스 index가 하나의 차원에 대한 검색 속도만 향상시킬 수 있다는 점이었다.  
  이는 자연스럽게 _"2차원의 데이터를 1차원에 담을 수 있을까?"_ 라는 질문으로 이어지는데, 이 질문의 정답은 **그렇다** 이다.

- 큰 관점에서 보면 geospatial indexing 접근법은 두 가지가 있는데, 바로 hash와 tree이다.

  - Hash: even grid, geohash, cartesian tiers 등
  - Tree: quadtree, Google S2, RTree 등

- 아래 그림은 geospatial index들의 종류를 나타내며, 그 중 가장 자주 사용되는 색칠된 방법들만 살펴보도록 하자.

![picture 11](/images/SDI2_PS_2.png)

- 위 방법들의 내부 구현 방식과 접근법은 모두 다르지만, 개략적 아이디어는 아래와 같이 모두 동일하다.
  - **"지도를 작은 지역들로 나누고, index를 만들어 빠른 검색을 가능하게 하자"**

#### Evenly divided grid

- 단순한 방법 중 하나로 전 세계를 동일한 크기의 작은 grid(격자무늬)로 분할할 수 있다. 이 방법을 사용하면 하나의 grid는 여러 개의 장소를 포함할  
  수 있으며, 각 장소는 하나의 grid에 포함되게 된다.

- 이 방법은 한 가지 큰 문제가 있는데, 바로 장소의 분배가 적절하게 이뤄지지 않는다는 것이다. 특정 지역에는 많은 장소들이 포함될 수도 있지만,  
  그와 반대로 몇 개의 장소 또는 아예 아무런 장소가 없는 지역도 존재하게 된다. 이렇게 전 세계를 동일한 grid로 나누는 것은 data의 분배를 적절히  
  해내지 못한다. 이상적으로 많은 장소가 밀집된 지역은 조금 더 작은 grid로, 장소가 별로 없는 지역은 큰 grid로 나눌 수도 있을 것이다.  
  이렇게 할 경우, 인접한 grid를 찾는 방법이 또다른 문제가 된다.

#### Geohash

- Geohash는 evenly distributed grid보다 더 나은 선택지이다. Geohash는 위도, 경도로 이뤄진 2차원의 데이터를 특정 단어들로 이뤄진  
  문자열과 숫자를 사용해 1차원의 데이터로 변환한다. Geohash 알고리즘은 반복적으로 bit를 추가하며 세상을 작은 grid로 축소하하는 방식으로  
  이뤄진다. 개략적으로 이 알고리즘이 어떻게 동작하는지 살펴보자.

- 첫째로 전 세계를 본초자오선(prime meridian)과 적도(equator)를 사용해 4개로 분할한다. 예를 들어, 아래처럼 나눌 수 있다.

  - 0: 위도 범위 [-90, 0]
  - 1: 위도 범위 [0, 90]
  - 0: 경도 범위 [-180, 0]
  - 1: 경도 범위 [0, 180]

- 위처럼 4분할하면 전 세계는 01, 11, 00, 10의 4개 grid로 분할된다.

- 두번째로 각 grid를 다시 4개의 작은 grid로 분할한다. 각 grid는 또다시 위도 범위, 경도 범위를 나타내는 bit가 추가되어 표현된다.

![picture 12](/images/SDI2_PS_3.png)

- Geohash는 이 과정을 각 grid가 요구사항에 맞는 위도, 경도 범위 단위로 나뉘어 질때까지 반복한다.  
  Geohash는 주로 base32 표현식을 사용한다. 두 가지 예시를 보자.

  - Google Headquarter의 geohash:

    - 1001 10110 01001 1000 11011 11010(base32를 2진수로 표현한 값) -> 9q9hvu(base32)

  - Facebook Headquarter의 geohash:
    - 1001 10110 01001 10001 10000 10111(base32를 2진수로 표현한 값) -> 9q9jhr(base32)

- Geohash는 아래 표 처럼 12 단계의 정밀도(precision = level)를 가지며, 정밀도는 각 grid의 크기를 나타낸다.  
  일반적으로 정밀도가 6보다 높으면 grid가 너무 작고, 4보다 작으면 너무 크기 때문에 4와 6사이의 정밀도를 선택한다.

| geohash length | Grid width \* height               |
| -------------- | ---------------------------------- |
| 1              | 5009.4km \* 4992.6km (지구의 크기) |
| 2              | 1252.3km \* 624.1km                |
| 3              | 156.5km \* 156km                   |
| 4              | 29.1km \* 19.5km                   |
| 5              | 4.9km \* 4.9km                     |
| 6              | 1.2km \* 609.4m                    |
| 7              | 152.9m \* 152.4m                   |
| 8              | 38.2m \* 19m                       |
| 9              | 4.8m \* 4.8m                       |
| 10             | 1.2m \* 59.5cm                     |
| 11             | 14.9cm \* 14.9cm                   |
| 12             | 3.7cm \* 1.9cm                     |

- 그렇다면 이 시스템의 경우 어떤 정밀도를 선택해야 할까? 사용자가 반경을 정의할 수 있지만, 기본 반경은 5000m라고 했다.  
  반경과 geohash의 정밀도 간의 관계는 아래와 같다.

| Radius(km) | geohash length |
| ---------- | -------------- |
| 0.5km      | 6              |
| 1km        | 5              |
| 2km        | 5              |
| 5km        | 4              |
| 20km       | 4              |

- Geohash를 사용하면 대부분의 경우 잘 처리가 되지만, geohash의 경계가 계산되는 방식에 따른 몇 가지 edge case들이 존재한다.

##### Boundary issues

- Geohash는 서로 다른 2개의 geohash에 대해 이 둘의 공통적인 prefix의 길이가 더 길수록 더 가까움을 보장한다.

- 문제 (1)

  - 위 성질은 무조건 보장되는 반면, 그 반대는 성립하지 않는다. 즉 두 개의 위치는 매우 가까울 수 있지만, 공통적인 prefix가 아예 없을 수도 있다.  
    이는 적도나 본초자오선을 두고 갈라진 서로 다른 2개의 위치는 서로 다른 기준선을 두고 나뉘어져 있기 때문이다. 예를 들어 프랑스의  
    La RocheChalis의 geohash는 u000이지만, 그로부터 30km 떨어진 Pomerol의 geohash는 ezzz이다. 즉, 매우 가깝지만 서로 공통적으로 갖는  
    prefix가 하나도 없다.

  - 이 문제 때문에 prefix를 사용하는 아래의 SQL문은 가까운 장소들을 가져오는 데에 사용할 수 없다.

    ```sql
    SELECT * FROM geohash_index WHERE geohash LIKE '9q8zn%';
    ```

- 문제 (2)

  - Boundary와 관련된 또다른 문제점은 서로 다른 2개의 위치가 긴 길이의 같은 prefix를 갖지만, 서로 다른 geohash에 속할 수 있다는 것이다.

  - 이를 해결하기 위해 가장 많이 사용되는 해결책으로 현재 grid 뿐만 아니라 인접한 grid의 정보들을 모두 가져올 수 있다.  
    특정 geohash의 인접 geohash들은 상수 시간 내에 계산 가능하다.

##### 장소 정보가 부족한 경우

- 만약 grid와 그로부터 인접한 grid를 모두 봤음에도 불구하고 장소 정보가 매우 적거나 없는 경우가 발생하면 어떻게 해야할까?  
  첫 번째 해결책으로 이를 단순히 무시하고 지정된 반경 내의 장소들만 반환할 수 있다. 이 방법은 구현하기에는 쉽지만, 정보가 부족하기 때문에  
  UX를 해칠 수 있다.

- 두 번째 해결책으로 검색 반경을 늘릴 수 있다. 첫 검색 결과에 충분한 수의 장소가 조회되지 않았다면, 해당 grid의 geohash의 마지막 숫자를  
  제거한 geohash를 사용해 장소를 찾을 수 있을 것이다. 이러한 방식으로 특정 장소 개수가 발견될 때까지 geohash 범위를 늘려나갈 수 있다.

![picture 13](/images/SDI2_PS_4.png)

#### Quadtree

- Geohash와 더불어 자주 사용되는 또다른 해결책으로 quadtree가 있다.  
  Quadtree는 2차원의 공간을 특정 조건을 만족할 때까지 4개의 grid로 반복적으로 나누는 데 사용되는 자료구조이다. 조건의 예시로 각 grid 내의  
  장소 개수가 100개 미만일 때까지 분할할 수 있다. Quadtree를 사용하면 질의를 답하기 위해 in-memory tree 구조를 만들어낸다.  
  즉, quadtree는 데이터베이스 솔루션이 아니라 in-memory 자료구조인 것이다. Quadtree는 LBS Server에서 사용되며, 이 자료구조는  
  서버가 처음 실행될 때 (start-up time) 만들어진다.

- 아래 그림은 전 세계가 2억 개의 장소들을 갖는다고 가정하고 quadtree를 만들어내는 간단한 모습을 보여준다.

![picture 14](/images/SDI2_PS_5.png)

- 아래 그림은 조금 더 자세히 quadtree의 생성 과정을 보여준다. Root node는 전세계의 지도를 나타낸다. Root node는 각 node 내의 장소  
  수가 100개가 미만이 될 때까지 반복적으로 4분할된다.

![picture 15](/images/SDI2_PS_6.png)

- Quadtree를 만들기 위한 pseudo code는 아래와 같다.

```java
public void buildQuadtree(TreeNode node) {
	if (countNumberOfBusinessesInCurrentGrid(node) > 100) {
		node.subdivide();
		for (TreeNode child : node.getChildren()) {
			buildQuadtree(child);
		}
	}
}
```

##### Quadtree를 구성하는 데 필요한 메모리 크기는 얼마나 될까?

- 위 질문에 대한 답을 알기 위해서는 어떤 데이터가 저장되는지를 먼저 알아야 한다.

- Leaf node의 데이터는 아래와 같다.

| Name                                            | Size                                              |
| :---------------------------------------------- | :------------------------------------------------ |
| grid 범위를 알기 위한 좌상단 좌표와 우하단 좌표 | 32bytes(8bytes \* 4)                              |
| grid 내의 business ID 목록                      | 8bytes(per ID) \* 100(1개 grid 내 최대 장소 개수) |
| Total                                           | 832bytes                                          |

- Leaf node가 아닌 다른 node(internal node)의 데이터는 아래와 같다.

| Name                                            | Size                 |
| :---------------------------------------------- | :------------------- |
| grid 범위를 알기 위한 좌상단 좌표와 우하단 좌표 | 32bytes(8bytes \* 4) |
| 4개의 children node를 가라키기 위한 포인터      | 32bytes(8bytes \* 4) |
| Total                                           | 64bytes              |

- 여기서 quadtree를 만들어내는 과정은 하나의 grid 내의 장소수를 몇 개로 둘 것이냐에 따라 달라진다.  
  이 개수는 데이터베이스에서 참조할 수 있으므로 quadtree node내에 저장할 필요는 없다.

- 자료구조에 대해 이해했으니, 메모리 사용량에 대해 파악해보자.

  - 각 grid는 최대 100개의 장소를 저장할 수 있다.
  - Lead node의 개수는 `200m / 100 = 2m`개 이다.
  - Internal node들의 개수는 `200m / 3 = 0.67m`개이다.
  - 전체적으로 필요한 메모리량은 `2m * 832bytes + 0.67 * 64bytes = 1.71GB`이다. Tree를 만들기 위해서 조금의 overhead가 추가되더라도  
    여전히 tree를 만들어내기 위해 필요한 메모리량은 적다.

- 그렇다면 quadtree를 만들어내기 위한 메모리량이 꽤나 적은 평닝니, 하나의 서버에 quadtree index를 저장하도록 하면 될까?  
  답은 아니다. Read volume의 크기에 따라 하나의 quadtree 서버는 모든 read 요청을 처리하기 위한 CPU가 부족하거나 네트워크 bandwidth가  
  낮을 수도 있다. 따라서 이런 경우를 대비해 조회 요청을 여러 개의 quadtree 서버들로 분산시킬 필요가 있다.

##### Quadtree를 만드는 데는 얼마나 걸릴까?

- 각 lead node는 대략 100개의 장소 정보(business ID)를 갖는다. 따라서 tree를 만들어내기 위한 시간복잡도는 `n/100 * log(n/100)`이다.

  > n: 전체 장소 수

- 따라서 200m 개의 장소들을 갖는 quadtree를 만들어내는 데는 꽤나 시간이 소요될 수 있을 것이다.

##### Quadtree를 사용해 가까운 장소들을 어떻게 조회할까?

- (1) 메모리에 quadtree를 만들어낸다.
- (2) Quadtree가 만들어지면 root node로부터 시작해 검색하는 위치가 포함된 leaf node를 찾을 때까지 tree를 순회한다.  
  만약 해당 leaf node가 100개의 장소를 갖는다면 그 정보를 반환하고, 그렇지 않다면 100개에 도달할 때까지 인접 node들의 장소 정보들까지 추가해나간다.

##### Quadtree를 운영할 때 고려할 사항들

- 위에서 봤듯이, 200m 개의 장소들을 담는 quadtree를 서버의 start-time에 만들어내는 것은 꽤나 시간이 소요된다. 이렇게 서버의 start-time이  
  오래 소요되는 것은 소프트웨어 시스템을 구축할 때 꼭 고려해야 하는 상황이다. 새로 서버가 실행되고 quadtree를 구축하는 동안에는 트래픽을 받아  
  처리할 수 없다. 따라서 한 번에 기존 서버를 제거하고 새로운 서버를 실행시키는 것이 아니라, 서버들 중 일부만을 새로운 버전으로 실행시키는  
  roll out 배포 방식으로 고려해야 한다. 이런 roll out 배포를 활용하면 서버 클러스터가 모두 offline이 되거나 부하를 받는 상황을 줄일 수 있다.  
  또다른 배포 방식으로 blue/green 배포 방식을 택할 수도 있는데, 이 방식을 사용하면 클러스터 내의 모든 새로운 서버가 데이터베이스로부터 200m개의  
  장소 정보를 조회하므로 데이터베이스에 꽤나 큰 부하를 일으킬 수 있다.

- 운영 시 고려해야할 또다른 사항으로 시간이 지나 장소 정보가 추가되거나 삭제되었을 때 이를 어떻게 반영해야 할지 생각해야 한다.  
  가장 쉬운 방법으로 한 번에 전체 서버 클러스터의 서버 중 일부의 quadtree를 단계적으로 rebuild할 수 있을 것이다.  
  하지만 이런 고민을 할 때는 항상 비즈니스 생각을 해야 한다. 이전에 요구사항에서 새롭게 추가되거나 삭제된 장소 정보는 다음 날에 반영되어야 한다고 했다.  
  따라서 새벽 시간대에 cache를 갱신해 처리해낼 수 있다. 한 번의 batch job으로 이를 처리했을 때 200m개의 key들이 한 번에 invalidate되기  
  때문에 cache server에 부하가 발생할 수 있다는 점도 고려해야 한다.

#### Google S2

- Googld S2는 geometry(기하학) 라이브러리로, 이 분야에서 자주 사용되는 또다른 선택지이다. S2는 quadtree와 같이 in-memory 솔루션이다.  
  S2는 Hilbert curve에 기반해 특정 영역을 1D index에 매핑한다. Hilbert curve는 매우 중요한 속성을 갖는데, 바로 Hilbert curve 상에  
  가까운 2개의 점들은 1D 공간에서도 가깝다는 것이다. 1차원 공간에서의 검색은 2차원 공간에서 검색하는 것보다 매우 효율적이다.

- S2는 매우 복잡한 라이브러리이고, 이 라이브러리의 세부 구현 사항을 인터뷰에서 논할 필요는 없다. 하지만 Google, Tinder 등 많은 회사에서 널리  
  채택된 기술이므로 장점을 알아두는 것은 좋다.

  - S2는 임의적인 영역에 대한 처리를 잘 할 수 있어 지리상의 위치나 특정 지역에 대한 가상의 경계(geofencing)를 표현하는 데 좋다.  
    Wikipedia에 따르면 geofence는 실 세계 지리 데이터의 가상 파라미터이다. Geofence는 특정 point location의 반경을 기준으로  
    동적으로 생성될 수 있고, 사전 정의된 경계들의 집합일 수도 있다.  
    이렇게 Geofencing을 활용하면 특정 영역을 둘러싸는 둘레를 정의할 수 있어 해당 영역 밖에 있는 사용자들에게도 알릴 수 있다.  
    이런 식으로 단순히 주변 장소들을 알려주는 것 뿐만 아니라 더 많은 기능들을 제공할 수 있다.

  - S2의 또다른 장점은 S2가 사용하는 Region Cover 알고리즘에 있다. Geohash처럼 고정된 정밀도(precision, level)을 가지는 대신  
    S2에서는 최소 level, 최대 lvel, 그리고 최대 cell 개수를 지정할 수 있다. 각 Cell의 크기는 서로 다를 수 있기 때문에  
    S2를 사용해 받는 결과는 더 세밀하다.

#### 선택지 중 결정하기

- 주변 장소들을 효과적으로 찾아내기 위해, 위에서 몇 가지 선택지를 살펴보았다.  
  아래 표에서 볼 수 있듯이, 많은 회사 및 기술에서는 각자 다른 기술을 채택해 사용한다.

| Geo Index                 | Companies                      |
| ------------------------- | ------------------------------ |
| geohash                   | Bing map, Redis, MongoDB, Lyft |
| quadtree                  | Yext                           |
| Both geohash and quadtree | Elasticsearch                  |
| S2                        | Google Maps, Tinder            |

- 우리는 여기서 **geohash 또는 quadtree**를 선택하도록 할 것이다. S2를 선택하지 않은 이유는 인터뷰 동안 설명하기에는 매우 복잡하게  
  구현되어 있기 때문이다.

#### Geohash vs quadtree

- 이 두 기술 중 어떤 것을 선택할지 결정하기 위해, 간단히 비교를 해보자.

##### Geohash

- 빠른 구현이 가능하고 사용하기 쉽다. Tree를 만들 필요도 없다.
- 주어진 반경 내의 장소들을 알아낼 수 있다.
- Geohash의 정밀도(precision, level)가 고정되면 grid의 크기도 고정된다. 따라서 밀집도에 따라 동적으로 grid의 크기를 변경시키는 등의  
  작업은 불가하다. 이를 위해서는 훨씬 더 복잡한 로직이 들어가야 한다.
- Index를 갱신하는 작업이 쉽다. 예를 들어, 특정 장소를 index로부터 제거하고 싶다면 단순히 해당 장소에 맞는 row를 geohash와  
  business_id에 따라 제거하기만 하면 된다.

##### Quadtree

- Tree 자료구조를 만들어내야 하기 때문에 구현하기가 조금 더 복잡하다.
- _k-nearest_ 장소들을 알아낼 수 있다. 예를 들어, 특정 경우에는 반경 내에 속하는지에 관계 없이 _k_ 개의 주변 장소들만 보고 싶을 것이다.  
  만약 자동차를 운전하고 있는데 기름이 부족하다면, _k_ 개의 가까운 주유소만 알아내면 된다. 비록 주유소 각각이 사용자와 완전히 가깝진 않을  
  수 있지만, 그래도 애플리케이션은 가까운 주유소 _k_ 개를 반환하게 된다. 이런 형식의 질의는 분할 과정이 _k_ 에 맞게 진행되고, _k_ 개의  
  결과가 나올 때까지 검색할 범위를 동적으로 확장해갈 수 있기 때문에 quadtree에 들어맞는다.
- 밀집도에 따라 grid의 크기를 동적으로 변경할 수 있다.
- Index를 갱신하는 작업은 geohash보다 더 복잡하다. Quadtree는 tree 자료구조이기 때문에 특정 장소가 제거된다면 root node부터 시작해  
  해당 장소의 id를 갖는 leaf node까지 순회해야 한다. 예를 들어 `business_id = 2`인 장소를 제거해야 한다면 아래 그림처럼  
  root node로부터 시작해 leaf node까지 순회해가야 한다. 이 작업은 `O(logn)`의 시간 복잡도를 갖지만, 만약 multi thread를  
  활용한다면 locking이 들어가야 하기 때문에 더 복잡해질 것이다. 또한 tree를 rebalance하는 것도 복잡하다. 만약 leaf node가  
  가득 차 새로운 장소를 추가할 수 없는 상태가 된다면, tree를 rebalance해야 할 것이다.

![picture 16](/images/SDI2_PS_7.png)

---

## 상세 설계

- 이제 시스템의 개략적 설계에 대해서는 어느 정도 파악이 되었다. 이번에는 아래의 항목들을 더 자세히 살펴보자.

  - 데이터베이스 확장
  - Caching
  - Region, Availability Zones
  - 시간 또는 장소의 종류에 따른 filtering
  - 최종 아키텍쳐

### 데이터베이스 확장

- 이번에는 가장 중요한 business, geospatial index table의 확장에 대해 다뤄보자.

#### Business table

- Business table에 포함되는 데이터는 하나의 서버에 저장되기에는 매우 많을 수 있다. 따라서 데이터베이스 sharding을 고려해야 한다.  
  가장 쉬운 방법으로 business_id에 따라 sharding을 진행할 수 있다. 이 방식을 선택하면 데이터가 여러 shard들에 거쳐 고르게 분배됨이  
  보장되고, 운영하기도 쉽다.

#### Geospatial index table

- Geohash와 quadtree는 모두 많이 사용되는 기술이다. Geohash의 단순함 덕분에 geohash를 사용하기로 결정했다고 해보자.  
  그러면 geospatial index table을 크게 두 가지 방식으로 구축할 수 있을 것이다.

- (1): 각 geohash key에 따라 business_id들을 담는 JSON array를 저장한다.  
  이는 즉 특정 geohash내의 모든 business_id들이 하나의 row에 저장될 것임을 의미한다.

| geospatial_index     |
| -------------------- |
| geohash              |
| list_of_business_ids |

- (2): 같은 geohash 내에 여러 장소들이 있다면, 이를 각각 하나의 row로 나타낸다. 즉 하나의 장소는 하나의 row로 표현된다.

| geospatial_index |
| ---------------- |
| geohash          |
| business_id      |

- 아래는 2번 선택지에 데이터가 들어가는 형태의 예시이다.

| geohash | business_id |
| ------- | ----------- |
| 32feac  | 343         |
| 32feac  | 347         |
| f31cad  | 112         |
| f31cad  | 112         |

- 1, 2번 선택지 중 어떤 것을 선택하는 것이 좋을까? 정해진 답은 없지만, 여기서는 아래의 이유들 때문에 2번 선택지를 사용할 것이다.

  - 1번 선택지에서 장소를 갱신하기 위해서는 `business_id`들을 담는 배열을 가져와 해당 배열을 모두 scan한 후 갱신할 장소를 찾아내야 한다.  
    새로운 장소를 추가할 때는 해당 장소가 추가될 배열을 전체 scan해 중복값이 없는지를 검증해야 한다. 동시적으로 갱신이 일어나는 것을 막기 위해  
    row를 lock해야 하기도 한다. 그리고 처리해야 할 edge case들이 많다.

  - 2번 선택지는 테이블이 `geohash`, `business_id`를 primary key로 갖는 복합 key를 사용한다. 따라서 장소의 수정이나 추가가 매우 쉽다.  
    또한 lock을 걸어야할 필요도 없다.

##### Geospatial index의 확장

- Geospatial index를 확장할 때 가장 많이 하는 실수 중 하나는 테이블의 실제 크기를 고려하지 않고 sharding 방식부터 고민한다는 것이다.  
  여기서 설계하련느 시스템의 경우, geospatial index table에 들어가는 데이터의 크기는 그렇게 크지 않다.  
  (quadtree index가 1.71GB를 차지하고, 이는 geohash index에서도 비슷하다.) 즉 전체 geospatial index가 하나의 데이터베이스 서버에  
  모두 저장되어도 문제되지 않는다. 하지만 이렇게 하나의 데이터베이스 서버만을 사용하면 read 연산량이 증가하면 모든 요청을 처리하기 위해 CPU가  
  부족하거나 network bandwidth가 부족할 수 있다. 따라서 여러 개의 데이터베이스 서버를 두고 read 요청을 분산하는 것도 고려해야 한다.

- 관계형 데이터베이스의 부하를 분산하는 방식으로는 크게 두 가지가 있는데, 바로 read replica를 활용하는 것과 sharding이다.

- Sharding은 충분히 좋은 방법이지만 꽤나 복잡한 작업이기 때문에 geohash table에 대해서는 부적합할 수 있다.  
  복잡한 작업인 이유 중 하나가 바로 sharding 로직이 application layer에 구현되어야 한다는 점이다.  
  그리고 지금 설계하는 시스템은 사실 한 대의 데이터베이스 서버에 데이터를 모두 저장해도 되기 때문에 데이터를 여러 서버에 분산시킬 필요가 크게 없다.

- 이 경우 더 나은 선택지는 read replica들을 추가해 read 연산들을 분산시키는 것이다. 이 방식은 훨씬 더 간단하며 운영하기도 쉽다.  
  따라서 geospatial index는 read replica 방식을 사용해 확장하도록 하자.

### Caching

- Cache layer에 대해 설계하기 전에 꼭 _"과연 우리가 진짜 cache layer가 필요한 상황일까?"_ 라는 질문을 항상 먼저 답해야 한다.

- 이 상황에서 caching을 도입하는 것이 항상 좋지만은 않다.

  - 대부분의 요청은 read 연산이고, 데이터의 크기가 상대적으로 작다. 데이터는 한 대의 데이터베이스 서버에 모두 저장될 수 있다.  
    따라서 query들은 I/O의 부하를 일으키지 않고 in-memory cache만큼 빠르게 수행될 것이다.

  - Read 연산에서 병목현상이 일어난다면 database read replica를 추가해 read throughput을 향상시킬 수 있다.

> 인터뷰 시 caching에 대해 이야기할 때는 benchmarking 및 cost analysis 이야기도 꼭 나오기 때문에 유의해야 한다.  
> 만약 caching이 비즈니스 요구사항을 만족시키기 위해 필요하다고 생각된다면, caching 전략부터 이야기를 해 나가면 된다.

#### Cache Key

- 가장 명료한 cache key는 바로 사용자의 위도, 경도가 될 것이다. 하지만 이 방식은 아래의 몇 가지 문제를 야기한다.

  - 모바일 기기에서 파악되는 위치 좌표 정보는 정확하지 않다. 사용자가 실제로 움직이지 않더라도 모바일 기기에서 위치 좌표를 조회했을 때 결과가 다를 수 있다.
  - 사용자가 위치 좌표가 매우 적게 변하는 정도만 움직일 수 있다. 대부분의 애플리케이션에서 이 정도의 변화는 별 의미가 없다.

- 따라서 위치 좌표 정보를 cache key로 갖는 것은 별로 좋은 선택지가 되지 못한다. 이상적으로 위치 좌표가 조금만 변경된다면, 변경 전과 후는 동일한  
  cache key로 매핑되어야 한다. 이전에 봤던 geohash, quadtree는 특정 grid내의 장소들을 같은 geohash에 매핑하기 때문에 이를 효율적으로 처리한다.

#### Caching할 데이터의 종류들

- 아래 표에 나타난 것처럼 시스템의 전체적인 성능을 개선하기 위해서는 두 가지 종류의 데이터를 caching해야 한다.

| Key         | Value                          |
| ----------- | ------------------------------ |
| geohash     | grid 내의 business_id의 리스트 |
| business_id | 장소 정보 객체                 |

##### Grid 내에 있는 business ID의 목록

- 장소 정보는 쉽게 추가, 수정, 삭제되지 않으므로 주어진 geohash내의 business ID들의 목록을 미리 구해놓고 Redis와 같은 key-value store에  
  저장해 놓을 수 있다.

- Caching이 적용된 상태로 주변 장소들을 가져오는 간단한 과정을 보자.

  - (1) 주어진 geohash 내의 business ID들을 가져온다.

    ```sql
    SELECT business_id FROM geohash_index WHERE geohash LIKE `{:geohash}%`;
    ```

  - (2) Cache miss가 발생한 경우, 결과를 Redis에 caching한다.
    ```java
    public List<String> getNearbyBusinessIds(String geohash) {
    String cacheKey = hash(geohash);
    List<String> listOfBusinessIds = Redis.get(cacheKey);
    if (listOfBusinessIds == null) {
    	listOfBusinessIds = /*Run the SELECT SQL Query above"*/;
    	Cache.set(cacheKey, listOfBusinessIds, "1d");
    }
    return listOfBusinessIds;
    }
    ```

- 새로운 장소가 추가, 수정 또는 삭제되었을 때 데이터베이스도 갱신되고 cache 또한 invalidate된다. 이 연산의 크기가 상대적으로 작고, geohash를  
  사용하면 locking 메커니즘도 필요 없기 때문에 갱신 연산을 처리해내기도 쉽다.

- 사전에 정의한 요구사항에 따르면 사용자는 500m, 1km, 2km, 5km의 4가지 검색 반경 범위를 선택할 수 있다.  
  이 반경 범위는 각각 geohash 정밀도 4, 5, 5, 6에 해당된다. 빠르게 검색 반경에 따라 주변 장소들을 반환할 수 있도록 하기 위해  
  Redis에 `geohash_4`, `geohash_5`, `geohash_6`을 모두 저장해 놓을 수 있을 것이다.

- 이전에 봤듯이 애플리케이션에는 총 200m개의 장소 정보가 있으며 각 장소는 주어진 정밀도에 따라 1개의 grid에 속하게 된다.  
  따라서 필요한 총 메모리량은 아래와 같다.

  - Redis value: `8bytes * 200m * 3precisions = 5GB`
  - Redis key: 무시할 수 있을 정도로 작다.
  - 필요한 총 메모리량: 대략 5GB

- 메모리 측면에서 봤을 때는 5GB 정도에 그치기 때문에 한 대의 Redis 서버에 모두 caching을 할 수 있을 것이다.  
  하지만 높은 가용성과 대륙 간의 latency를 줄이기 위해서 Redis cluster를 전 세계에 걸쳐 배포할 수 있을 것이다.  
  5GB 정도면 동일한 cache data를 갖는 여럿 redis server를 배포하기가 전혀 어렵지 않다.

#### 클라이언트 페이지를 렌더링하기 위해 필요한 장소 상세 정보

- 장소의 상세 정보는 caching하기 꽤나 단순하다. 이전에 봤듯이 key를 `business_id`로 두고, value를 해당 장소의 이름, 주소,  
  이미지 URL 등을 갖는 객체로 지정하면 된다.

### Region, Availability Zones

- LBS는 여러 개의 region과 AZ에 걸쳐 배포하는 것이 좋다. 이런 방식의 장점은 아래와 같다.

  - 사용자가 물리적으로 시스템에 _가까이_ 위치할 수 있도록 한다. 예를 들어 US West에 있는 사용자들은 해당 region에 있는  
    데이터 센터와 소통하며, Europe에 있는 사용자들은 Europe의 데이터 센터와 소통한다.

  - 인구 수에 따라 트래픽을 적절히 분배할 수 있게 된다. Japan, Korea와 같은 region은 인구 밀집도가 높다.  
    따라서 이 둘을 서로 다른 region에 있는 데이터 센터에 각각 소통하게 하거나 AZ 다중화를 진행해 부하를 분산시킬 수 있다.

  - 특정 나라들은 사용자 데이터가 local하게 저장되는 것을 법으로 요구하기도 한다. 이 경우, 해당 나라 안의 region에 시스템을  
    배포해 해당 나라에서 발생하는 모든 트래픽을 해당 나라 안의 region으로 routing할 수 있을 것이다.

### 시간 또는 장소의 종류에 따른 filtering

- 만약 현재 영업 중인 장소나 음식점만 반환하는 등의 조건들이 추가된다면 어떻게 해야 할까?

- Geohash나 quadtree를 사용해 전 세계가 작은 grid들로 분할되면, 검색 결과로 반환되는 장소들의 개수가 상대적으로 적을 것이다.  
  따라서 검색 조건에 맞는 business_id 들을 모두 가져온 후 각 id에 맞는 장소 정보 객체를 가져와 필터링을 적용해도 괜찮을 것이다.

### 최종 아키텍쳐

- 최종 아키텍쳐는 아래와 같다.

![picture 17](/images/SDI2_PS_8.png)

#### 주변 장소 조회

- 주변 장소를 조회할 때의 순서는 아래와 같다.

  - (1) 클라이언트가 위도, 경도와 함께 검색 반경을 Load balancer에게 전달한다.
  - (2) Load Balancer는 요청을 LBS에게 전달한다.
  - (3) LBS는 사용자 위치 좌표 정보와 반경에 따라 적절한 geohash length를 선택한다.  
    예를 들어 검색 반경이 500m인 경우, level 6를 선택한다.
  - (4) LBS는 인접한 geohash들을 찾아내고, 이들을 리스트에 추가한다. 결과는 아래와 같다.  
    `list_of_geohashes = [my_geohash, neighbor_geohash_1, neighbor_geohash_2, ...]`
  - (5) `list_of_geohashes` 내에 있는 각 geohash에 대해 LBS는 Geohash를 담고 있는 Redis server에게  
    해당 geohash 내의 business_id들을 알아내기 위해 질의한다. 각 geohash 내의 business_id들을 알아내기 위한  
    질의는 병렬적으로 수행될 수 있다.
  - (6) Business ID들의 목록을 토대로 LBS는 장소 정보를 담는 "Business Info" Redis server에 질의한다.  
    반환된 정보들과 함께 각 장소와 사용자의 거리를 토대로 순위를 매겨 정렬한 후 클라이언트에게 결과를 반환한다.

#### 장소 정보 조회, 수정, 추가 또는 삭제

- 장소 정보와 관련된 모든 API는 LBS로부터 분리되어 처리된다. 예를 들어 특정 장소의 상세 정보를 알고 싶다면, business service는  
  먼저 "Business Info" Redis server에 질의해 caching된 정보가 있는지를 파악한다. Caching되어 있다면, cache되어 있는 데이터가  
  바로 클라이언트게에 반환된다. 그렇지 않다면 데이터는 database cluster에 질의해 가져오며, 반환된 데이터를 우선 redis에 caching한 후  
  결과를 클라이언트에게 반환한다. 이렇게 이후 질의가 database cluster로 가지 않고 cache된 데이터를 사용하도록 한다.

---

## 마무리

- 이번 장에서는 Proximity Service를 설계하는 과정을 살펴보았다.  
  이 시스템은 geospatial indexing을 요구하는 전형적인 LBS 이다.

- Geospatial indexing을 위해 아래의 선택지들을 살펴보았다.

  - Two-demensional Search
  - Evenly divided grid
  - Geohash
  - Quadtree
  - Google S2

- Geohash, Quadtree, S2는 모두 많은 회사들에서 채택되어 사용되는 기술이다. 이 장에서는 geohash를 선택했다.

- 상세 설계 부분에서는 caching이 왜 latency를 줄이는 데에 효과적인지를 봤으며, 어떤 데이터가 cache 되어야 하며 주변 장소들을 빠르게  
  조회하기 위해 어떻게 cache를 활용할지도 살펴보았다. 또한 데이터베이스의 replication(다중화)과 sharding도 살펴보았다.

- 다음으로는 LBS를 여러 region과 AZ에 걸쳐 배포함으로서 어떻게 가용성을 높이고, 사용자들을 물리적으로 서버와 가깝게 유지하며  
  지역 법규에 따라 데이터를 보호할 수 있는지도 살펴보았다.

---
